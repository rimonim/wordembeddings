% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fcm_operations.R
\name{fcm_pmi}
\alias{fcm_pmi}
\alias{fcm_smooth}
\alias{fcm_positive}
\alias{fcm_log}
\title{Transform a feature co-occurrence matrix}
\usage{
fcm_pmi(
  fcm,
  positive = TRUE,
  context_smoothing = 1,
  target_smoothing = 1,
  shift = 0,
  base = 2,
  prob = NULL
)

fcm_smooth(fcm, method = "goodturing", crit = 1.96, estimate_zeros = TRUE)

fcm_positive(fcm)

fcm_log(fcm, positive = TRUE, base = 2)
}
\arguments{
\item{fcm}{a \link[quanteda:fcm]{Quanteda fcm} or similar 2D matrix-like or 3D array-like object}

\item{positive}{logical. If \code{TRUE}, all negative elements are replaced with zeros.}

\item{context_smoothing}{numeric. Power to raise context (column) frequencies. Default is 1.
Mikolov et al. (2013) found 0.75 to work well for word2vec.}

\item{target_smoothing}{numeric. Power to raise target word (row) frequencies. Default is 1.
Set to 0 for uniform target weighting.}

\item{shift}{numeric. A number added to each element of the output (see details).}

\item{base}{the base with which to compute logarithms}

\item{prob}{\code{"rows"} indicates that elements are row probabilities (i.e. row sums
are assumed to be 1). \code{"cols"} indicates that elements are column probabilities
(i.e. column sums are assumed to be 1). If \code{NULL} (the default), elements are
taken as raw counts.}

\item{method}{smoothing method:
\describe{
\item{\code{goodturing}}{the "Simple Good-Turing" algorithm described by Gale &
Sampson (1995)}
\item{\code{laplace}}{Laplace (a.k.a. "add one") smoothing}
}}

\item{crit}{criterion for switching between raw and smoothed estimates when
\code{method = "goodturing"}. The default \code{1.96} corresponds to the standard
0.05 significance criterion.}

\item{estimate_zeros}{logical; if \code{TRUE}, distribute the estimated
probability of unobserved tokens among features with a count of zero.
Note that this coerces the output to a dense matrix, so may exceed memory
requirements for large FCMs.}
}
\description{
\code{fcm_pmi()} calculates the Pointwise Mutual Information between each word and
context (i.e. row and column) of the matrix. \code{fcm_log()} takes the logarithm
of each element. \code{fcm_smooth()} re-weights observed co-occurrence frequencies
to account for unobserved values. \code{fcm_positive()} sets all negative values to
zero.
}
\details{
If \code{fcm} is a 3D array or SparseArray, it is taken as a stack of feature
co-occurrence matrices. In other words, PMI is calculated for rows and
columns within each level of the third dimension.

\strong{Smoothing Parameters:}
Both \code{context_smoothing} and \code{target_smoothing} control how frequencies are
weighted when computing PMI. Context counts (columns) are raised to the power
of \code{context_smoothing}, and target word counts (rows) are raised to the power
of \code{target_smoothing}. Mikolov et al. (2013) found \code{context_smoothing = 0.75}
to work well for word2vec. Setting either parameter to 0 results in uniform
weighting for that dimension.

The shift parameter controls the prior probability of observing a true
co-occurence as opposed to a randomly sampled ("negative") one.
Thus \code{shift = -log(k, base)} is analogous to the Skip-gram algorithm with k
negative samples. \code{shift = log(sum(fcm), base)} is analogous to the GloVe
algorithm with weights fixed at log frequency.
}
\references{
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111â€“3119.
}
